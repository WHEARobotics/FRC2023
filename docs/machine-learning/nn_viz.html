<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Visualizing Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="nn_viz_files/libs/clipboard/clipboard.min.js"></script>
<script src="nn_viz_files/libs/quarto-html/quarto.js"></script>
<script src="nn_viz_files/libs/quarto-html/popper.min.js"></script>
<script src="nn_viz_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="nn_viz_files/libs/quarto-html/anchor.min.js"></script>
<link href="nn_viz_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="nn_viz_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="nn_viz_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="nn_viz_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="nn_viz_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Visualizing Neural Networks</h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="visualizing-the-3-layer-xor-neural-network" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-the-3-layer-xor-neural-network">Visualizing The 3-Layer XOR Neural Network</h2>
<p>In <code>01-xor.py</code>, we visualized the 3-layer XOR neural network like <strong>?@fig-xor</strong>:</p>
<p>In this figure, the <code>lin1</code> lines represent the weights connecting the input nodes (circles) to the nodes where we applied the sigmoid function (<span class="math inline">\(\sigma\)</span>), and the <code>lin2</code> lines represent the weights connecting the middle layer’s outputs to the single node in the output layer.</p>
<p>When every node in a layer is connected to every node in the output layer, it’s called a “fully-connected” layer or a “linear” layer. You can visualize <em>somewhat</em> larger networks including the weight connections:</p>
<p><img src="media/4layerlinear.png" class="img-fluid"></p>
<p>But drawing all the weights quickly becomes unmanageable. Instead, neural networks are typically described just drawing the layers:</p>
<p><img src="media/xorblock.svg" class="img-fluid"></p>
<p>Or even just:</p>
<p><img src="media/xorblock2.svg" class="img-fluid"></p>
<p>When we did the MNIST hand-writing challenge, we switched to a 28x28 black-and-white input, a 500-node middle layer, and a 10-node output layer. To visualize that, we might draw (forgive my isometric clumsiness):</p>
<p><img src="media/mnistblock_out.svg" class="img-fluid"></p>
</section>
<section id="layers-are-theoretically-sufficient-but-often-dont-scale" class="level2">
<h2 class="anchored" data-anchor-id="layers-are-theoretically-sufficient-but-often-dont-scale">3 Layers Are Theoretically Sufficient, But Often Don’t Scale</h2>
<p>To fully connect a 28x28 input with 500 nodes requires 28x28x500 weights. Add in a bias weight for each middle-layer node, and that’s 392,500 weights.</p>
<p>Fully connecting the 500 middle-layer nodes to 10 output nodes adds another 5,000 weights.</p>
<p>We were still able to train this on small laptops, but the number of middle-layer nodes in a 3-layer neural network tends to increase very fast as the problem gets harder. If we were to try to extend hand-writing recognition from 10 digits to 26 letters (2.6 times harder, perhaps), we might have to increase the number of middle-layer nodes, not to 500x2.6, but maybe to 5000.</p>
</section>
<section id="convolutional-layers" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-layers">Convolutional Layers</h2>
<p>What if, instead of the input being the full 28x28 pixel image, you had a 3-pixel by 3-pixel window that you slid over the 28x28 MNIST input and you mapped the result to a single output node?:</p>
<p><img src="media/convolution.svg" class="img-fluid"></p>
<p>You only have a 3x3 input and a size 1 output, so that’s just 9 weights! Add the bias and it’s still tiny! Now, you slide the window 28 times in a row and after each row you just slide it down 1, so you’re going to end up with an output the same size as your input:</p>
<p><img src="media/convolution_block.svg" class="img-fluid"></p>
<p>True, the middle layer is bigger than we had with 3 layers (28x28 = 784 nodes vs.&nbsp;500 nodes), but the number of weights is just 10!</p>
<p>This is what’s called a “convolution layer”. In this case, it’s a 2D convolution layer because it’s sliding a window over-and-down a 2D image. You can imagine that if you had a video file, you could do a 3D convolution where you looked at 3 pixels by 3 pixels by 3 frames.</p>
<p>Instead of just having one set of weights (one 3x3 <strong>kernel</strong>) it’s common to have several sets of weights. So if instead of 1 <strong>output channel</strong> you have 6, instead of 10 weights, you’d have 60. Still vastly, vastly smaller than the Linear layer!</p>
<section id="what-do-nodes-in-a-convolution-layer-react-to" class="level3">
<h3 class="anchored" data-anchor-id="what-do-nodes-in-a-convolution-layer-react-to">What do nodes in a convolution layer react to?</h3>
<p>A single node in a convolution layer only “sees” a 3x3 input. (Or 5x5, or 7x7, etc. But it’s often 3x3).</p>
<p>Imagine that, like our XOR network, the inputs were only 0 or 1 (instead of shades of gray, as they <em>really</em> are in MNIST). And imagine that the weights you had were:</p>
<p><span class="math display">\[
\left[
\begin{array}{ccc}
0.5 &amp; -1.0 &amp; -1.0 \\
-1.0 &amp; 0.5 &amp; -1.0 \\
-1.0 &amp; -1.0 &amp; 0.5
\end{array}
\right]
\]</span></p>
<p>The small set of weights you use when you slide the window over your total input is called the <strong>convolution kernel</strong>.</p>
<p>And what we’re doing is the same-old, same-old:</p>
<ul>
<li>multiply our inputs by our weights,</li>
<li>summing that,</li>
<li>and applying an activation function like <code>tanh</code> to the result</li>
</ul>
<p>So what are the kinds of value we get from this kernel? Let’s say the input is:</p>
<p><img src="media/convolution_kernel.svg" class="img-fluid"></p>
<p>And we count the black pixels as 1.0 and the blank pixels as 0.0, then our math becomes:</p>
<p><span class="math display">\[
\tanh(
\left[
\begin{array}{ccc}
0.5 \times 1.0 &amp; -1.0 \times 0 &amp; -1.0 \times 0 \\
-1.0 \times 0 &amp; 0.5 \times 1.0 &amp; -1.0 \times 0 \\
-1.0 \times 0 &amp; -1.0 \times 0 &amp; 0.5 \times 1.0
\end{array}
\right]
) = \tanh(1.5) = 0.91
\]</span></p>
<hr>
<p>Compare that with this input:</p>
<p><img src="media/conv_input2.svg" class="img-fluid"></p>
<p>The black pixels get multiplied by -1.0, and we end up with <span class="math display">\[ \tanh(-3) = -0.995 \]</span>.</p>
<p>In other words, we’ve created a “top-left, lower-right diagonal slash” detector:</p>
<p><img src="media/convolution_kernel2.svg" class="img-fluid"></p>
<section id="convolution-layers-create-feature-maps" class="level4">
<h4 class="anchored" data-anchor-id="convolution-layers-create-feature-maps">Convolution layers create “Feature Maps”</h4>
<p>When you take a certain input, multiply it by the weights in the kernel, and apply an activation function to the sum, the resulting number (the <strong>activation level</strong> of the <strong>convolutional node</strong>) will be higher for some patterns than for others. If the weights are set “correctly” (via slow modification via gradient descent, etc.), the kernel is a <strong>feature detector</strong>. The results of a convolutional layer are often called <strong>feature maps</strong>.</p>
<p>The kernel we showed above detects “upper-left to lower-right diagonals.” If we ran this over a 28x28 input, we’d have a 28x28 set of activations. Anywhere they were near 1.0, it would indicate that the 3x3 pixel area “above” that point “looked like” it had a backslash. Where the activation levels are near 0, it would indicate “nothing like a backslash.” Even if the input were:</p>
<p><span class="math display">\[
\left[
\begin{array}{ccc}
\blacksquare &amp; \square &amp; \square \\
\square &amp; \square &amp; \square \\
\square &amp; \square &amp; \blacksquare
\end{array}
\right] \times
\left[
\begin{array}{ccc}
0.5 &amp; -1.0 &amp; -1.0 \\
-1.0 &amp; 0.5 &amp; -1.0 \\
-1.0 &amp; -1.0 &amp; 0.5
\end{array}
\right]
\]</span></p>
<p>The activation would be <span class="math display">\[ \tanh(0.5 + 0.5) = 0.76 \]</span> “Kinda’ like the feature, but not fully.”</p>
<p>On the other hand, those -1 weights mean that any <em>other</em> black squares in the input will pretty-much guarantee low activation:</p>
<p><span class="math display">\[
\left[
\begin{array}{ccc}
\blacksquare &amp; \square &amp; \blacksquare \\
\square &amp; \square &amp; \square \\
\square &amp; \square &amp; \blacksquare
\end{array}
\right] \times
\left[
\begin{array}{ccc}
0.5 &amp; -1.0 &amp; -1.0 \\
-1.0 &amp; 0.5 &amp; -1.0 \\
-1.0 &amp; -1.0 &amp; 0.5
\end{array}
\right]
= \tanh(0.5 + -1.0 + 0.5) = \tanh(0) = 0.0
\]</span></p>
</section>
</section>
</section>
<section id="deep-feature-maps" class="level2">
<h2 class="anchored" data-anchor-id="deep-feature-maps">Deep Feature Maps</h2>
<p>A <strong>Deep Neural Network</strong> is any Artificial Neural Network (<strong>ANN</strong>) that has more than 3 layers.</p>
<p>If you have a first convolutional layer that has a <strong>receptive field</strong> of 3x3 pixels and your convolve over a 28x28 image, you end up with a 28x28 feature map. (You have to pad the edges with 0s or you’d actually end up with a 26x26 map, but it’s normal to pad.)</p>
<p>What happens when you do a convolution over <em>that</em> feature map?</p>
<p><img src="media/fmap_2.svg" class="img-fluid"></p>
<p>Remember that “Feature Map 1” is a bunch of “3x3 pixel feature detectors” . “Feature Map 2” also has a 3x3 receptive field, but each of its inputs is a “3x3 pixel feature detector.” So you could either call it:</p>
<ul>
<li>A map of 3x3 detectors of 3x3 features (more accurate)</li>
<li>A map of 9x9 features (less accurate, since it’s not looking at every pixel)</li>
</ul>
<p>A third convolutional layer would give us something that was sorta’ “looking at” a 27x27 area, a fourth one sorta’ looking at 81x81 pixels, etc.</p>
<section id="crucial-concept-deep-neural-networks-move-from-low-abstraction-to-high-abstraction" class="level3">
<h3 class="anchored" data-anchor-id="crucial-concept-deep-neural-networks-move-from-low-abstraction-to-high-abstraction">Crucial Concept: Deep neural networks move from low abstraction to high abstraction</h3>
<p>Only the first feature map is reacting to <em>every single pixel</em> in the input. It only “sees” very small details and so can only react to very small patterns.</p>
<p>The second feature map doesn’t see every single pixel, but it sees every single very small pattern in the first feature map. Many layers deep, the weights are evaluating very abstract features.</p>
<p><img src="media/abstraction.svg" class="img-fluid"></p>
</section>
</section>
<section id="pooling-layers" class="level2">
<h2 class="anchored" data-anchor-id="pooling-layers">Pooling Layers</h2>
<p>You could certainly fit many 28x28 convolutional layers into your computer memory, but, in most cases, you’re not really paying attention to <em>every</em> 3x3 pixel block, you only want to pay attention to “interesting” ones. If your weights are set to their trained values, “interesting” inputs will be those that generate values closer to -1 or 1 rather than 0. (Or, with other activation functions, maybe those with values closer to 0 or 1 rather than 0.5.)</p>
<p>Convolutional layers are generally paired with a <strong>Pooling layer</strong> that also slides a window over the input. This time, though, the <strong>receptive field</strong> is generally just 2x2.</p>
<p>And instead of having anything to do with weights, the pooling layer just selects, from among its inputs, the one with the greatest magnitude:</p>
<p><img src="media/pooling_kernel.svg" class="img-fluid"></p>
<p>A pooling layer with a 2x2 receptive field cuts the size of its input layer to a quarter of it’s original size (a half in each direction). One with a 3x3 receptive field cuts it by 8/9s! (2/3s in each direction).</p>
<p>Pooling layers are very commonly used with convolutional layers.</p>
</section>
<section id="a-deep-convolutional-neural-network-for-mnist" class="level2">
<h2 class="anchored" data-anchor-id="a-deep-convolutional-neural-network-for-mnist">A Deep Convolutional Neural Network for MNIST</h2>
<p>I don’t have time to go over this step by step, so I’m just going to present the code quickly.</p>
<p>You’ll see that, just as we defined a new class for our Xor network and our 3-layer MNIST network, we’re doing the same here.</p>
<p>To make the code a little shorter, I define <code>conv1</code> and <code>conv2</code> as layers that themselves each have a convolutional layer (<code>Conv2d</code>), use the <code>ReLU</code> activation function, and a pooling layer (<code>MaxPool2D</code>). Creating <strong>blocks</strong> like this that internally have layers and which, in turn, you layer with other blocks, is how deep-learning <strong>architectures</strong> are built.</p>
<section id="parameters-to-conv2d" class="level3">
<h3 class="anchored" data-anchor-id="parameters-to-conv2d">Parameters to <code>Conv2d</code></h3>
<p>When defining a convolutional layer (<code>Conv2d</code>) in Pytorch, the depth of the input is the number of <code>in_channels</code>. With a black-and-white image, we only have 1 input per pixel. If it were an RGB image with 3 values defining the pixel, the <code>in_channels</code> would be 3.</p>
<p>The <code>out_channels</code> is the number of kernels you want. Generally, you want to train a bunch of kernels, since each kernel has a receptive field of only <code>kernel_size * kernel_size</code> weights. (And remember that you’ll also have 1 bias weight for each output.)</p>
<p>Although we talked about 3x3 pixel stuff above, I happen to know that it takes a really long time to train a 3x3 convnet on MNIST. Instead, we use a <code>kernel_size</code> (really “length of one side of kernel”) of 5.</p>
<p>How much do you slide the window at each step? We set <code>stride</code> to 1, meaning that we just move the window 1 pixel over until we hit the end of the row and then go down 1. Setting it to a higher number will miss some of the small-scale patterns in your input, but will make for a smaller feature map. (I’m not sure I’ve <em>ever</em> seen a production system where <code>stride</code> &gt; 1 was used.)</p>
<p>What happens when your window hits the edge? Generally, you use <code>padding</code> to just put 0s on “the outside.” Since our <code>kernel_size</code> is 5, that means that one when the middle pixel is on the edge, there are 2 pixels “outside.” Thus, <code>padding = 2</code>.</p>
</section>
<section id="parameters-to-maxpool2d" class="level3">
<h3 class="anchored" data-anchor-id="parameters-to-maxpool2d">Parameters to <code>MaxPool2d</code></h3>
<p>If you understood the discussion of pooling layers, this should be pretty clear: our pooling layer has a receptive field of 2x2. It cuts the size of the feature map by 3/4, saving only the activation that has the highest absolute value.</p>
</section>
<section id="parameters-to-linear" class="level3">
<h3 class="anchored" data-anchor-id="parameters-to-linear">Parameters to <code>Linear</code></h3>
<p>The convolutional layers detect the features of the input, but we need to map those into one of 10 output classes (the digits from 0-9). Going through the “shapes” of the input size, the convolutional and pooling layers, the output of the second pooling layer is [32,7,7] meaning that I need 32x7x7 weights for each of the 10 outputs (plus 1 bias weight for each output value). We create a fully-connected, aka Linear, layer: 32 x 7 x 7 x 10 + 10 = 15,690 weights.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MnistConv(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MnistConv, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Sequential(</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>                in_channels <span class="op">=</span> <span class="dv">1</span>, <span class="co"># Black-and-white, single value</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>                out_channels <span class="op">=</span> <span class="dv">16</span>, </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                kernel_size <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                stride <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                padding <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># After pooling, output is 14x14x6 </span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Sequential(</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>                in_channels <span class="op">=</span> <span class="dv">16</span>,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>                out_channels <span class="op">=</span> <span class="dv">32</span>,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>                kernel_size <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>                stride <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>                padding <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(kernel_size <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># After pooling, output is 7x7x32</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="dv">7</span><span class="op">*</span><span class="dv">7</span><span class="op">*</span><span class="dv">32</span>, <span class="dv">10</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Flatten the output shape of conv2 to shape of output layer. Let PyTorch do the calc.</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.out(x)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> MnistConv()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This is not a <em>very</em> deep neural network, lol. It’s really just 6 layers:</p>
<ol type="1">
<li>Input layer (28 x 28 pixels, only 1 byte per pixel: 1x28x28)</li>
<li>Feature Map 1 (5x5 receptive field, 28 x 28 features, 16 kernels: 16x28x28)</li>
<li>Pooling layer 1 (Reduces size to 16x14x14)</li>
<li>Feature Map 2 (5x5 receptive field, 14 x 14 features, 32 kernels: 32x14x14)</li>
<li>Pooling layer 2 (Reduces size to 32x7x7)</li>
<li>Classification layer (Fully connects pooling layer 2 to 10 output values: 1568x10)</li>
</ol>
<p><img src="media/mnist_conv.svg" class="img-fluid"></p>
<p>It has fewer than 30,000 weights, that’s 1/10th the size of our 3-layer MNIST neural network.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>torchsummary.summary(net, (<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This is the <em>exact same</em> training loop as we used on 3-layer MNIST. Not a line of code is different:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):  <span class="co"># loop over the dataset multiple times</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    epoch_start <span class="op">=</span> time.time()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, data <span class="kw">in</span> <span class="bu">enumerate</span>(trainloader, <span class="dv">0</span>):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get the inputs; data is a list of [inputs, labels]</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        inputs, labels <span class="op">=</span> data</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># zero the parameter gradients</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward + backward + optimize</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> net(inputs)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        losses.append(loss)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print statistics</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        seconds <span class="op">=</span> time.time() <span class="op">-</span> epoch_start</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> : loss = </span><span class="sc">{</span>running_loss<span class="op">/</span><span class="bu">len</span>(trainset)<span class="sc">:.6f}</span><span class="ss">, took </span><span class="sc">{</span>seconds<span class="sc">:.1f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You may remember this hard-to-get target:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_and_tell(net, dataset, index):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    query_image, ground_truth <span class="op">=</span> dataset[index]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The image is now a PyTorch Tensor, so you need to convert it to a NumPy array first</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> query_image.numpy()</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The data is a 1x28x28 array, you need to remove the extra dimension</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> image.squeeze()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display the image and its label</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    plt.imshow(image, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Label: </span><span class="sc">%i</span><span class="st">'</span> <span class="op">%</span> ground_truth)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    prediction <span class="op">=</span> prediction_for_image(net, query_image)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Prediction is </span><span class="ch">\"</span><span class="sc">{</span>prediction<span class="sc">}</span><span class="ch">\"</span><span class="ss">, ground truth is </span><span class="ch">\"</span><span class="sc">{</span>ground_truth<span class="sc">}</span><span class="ch">\"</span><span class="ss">"</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># testset[478] is a hard one! Your model may get it wrong!</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>show_and_tell(net, testset, <span class="dv">478</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="whats-our-overall-accuracy" class="level2">
<h2 class="anchored" data-anchor-id="whats-our-overall-accuracy">What’s our overall accuracy?</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> np.array([])</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>net.<span class="bu">eval</span>()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, data <span class="kw">in</span> <span class="bu">enumerate</span>(testloader, <span class="dv">0</span>):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get the inputs; data is a list of [inputs, labels]</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    inputs, ground_truths <span class="op">=</span> data</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> net(inputs)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    batch_loss <span class="op">=</span> torch.eq(predictions.argmax(dim<span class="op">=</span><span class="dv">1</span>), ground_truths.argmax(dim<span class="op">=</span><span class="dv">0</span>)).<span class="bu">float</span>()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> np.append(loss, batch_loss.detach().numpy())</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> np.mean(loss.flatten())</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Across </span><span class="sc">{</span><span class="bu">len</span>(testloader) <span class="op">*</span> batch_size<span class="sc">}</span><span class="ss"> test images, accuracy is </span><span class="sc">{</span>accuracy<span class="sc">:.2%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>Although neural networks essentially consist of huge numbers of weights interconnecting layers, it’s impossible to diagram the interconnections. Instead, neural networks are discussed using block diagrams, with the connections implied.</p>
<p>Although 3-layer neural networks are theoretically able to compute any computable function, the number of weights needed scales poorly.</p>
<p>For image (and audio) (and sequence prediction), <strong>convolutional layers</strong> slide a window over the input, creating a <strong>feature map</strong> from very few weights. Because there are so few weights, you generally train several different <strong>kernels</strong> at once, generating several <strong>output channels</strong>. (For instance, the first convolutional layer in the MNIST network generates 16 channels.)</p>
<p><strong>Pooling layers</strong> select the strongest signal in their <strong>receptive field</strong>, discarding the rest. This reduces the size of the layers.</p>
<p>The “deeper” you go in a neural network, the harder it is to map a node’s value into a particular value in the input. What makes the value high? Well, the weights coming in and their inputs. What set those inputs? Well, the weights coming into <em>that</em> layer and <em>their</em> inputs. etc.</p>
<p>By the time you’re more than a handful of layers in, most of the pixels in the input have <em>something</em> to do with the activation of every node in a layer. This makes it very difficult to audit a neural model to make sure it’s making predictions “the right way,” or “using the right data.” This is known as the <strong>Interpretability Problem.</strong></p>
<p>In the case of MNIST, the convolutional neural network performs with essentially the same accuracy as the 3-layer neural network: generally achieving 92-93% accuracy. This is despite having fewer than 10% of the weights, which effectively means using much less memory. On the other hand, this deeper neural network trains a little slower, as convolving (sliding the window) is an inner loop that has to happen (even if we don’t have to explicitly write the code). The depth also means that the code has a little less <strong>cache coherence</strong>, which is a factor in performance tuning.</p>
<p>Pragmatically, the weight/memory savings <em>vastly</em> outweighs the slight speed hit. Convolutional neural networks are the standard architecture for neural network-based vision applications.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>