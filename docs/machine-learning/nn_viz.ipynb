{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1e00c4da-1ec9-4402-a28b-6ad4b241b3a8",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Visualizing Neural Networks\"\n",
    "execute:\n",
    "    echo: false\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "  pdf: \n",
    "    code-fold: true\n",
    "jupyter: manta\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58290c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776c2d1-722b-4e14-8b78-4b3f50f45bd4",
   "metadata": {},
   "source": [
    "## Visualizing The 3-Layer XOR Neural Network\n",
    "\n",
    "In `01-xor.py`, we visualized the 3-layer XOR neural network like @fig-xor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c0515-9e15-416a-a0b4-032b21d44ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\" width=\"450\" height=\"350\">   <circle cx=\"100\" cy=\"50\" r=\"4\" stroke=\"red\" stroke-width=\"2\" fill=\"none\"/>  <circle cx=\"250\" cy=\"50\" r=\"4\" stroke=\"red\" stroke-width=\"2\" fill=\"none\"/>  <text x=\"92\" y=\"205\" font-size=\"24pt\">σ</text>  <text x=\"240\" y=\"205\" font-size=\"24pt\">σ</text>  <circle cx=\"175\" cy=\"350\" r=\"4\" stroke=\"red\" stroke-width=\"2\" fill=\"none\"/>  <text x=\"300\" y=\"125\" stroke=\"red\">1.0 (&quot;Bias&quot; weights)</text>   <line x1=\"295\" y1=\"130\" x2=\"110\" y2=\"195\" stroke=\"red\" stroke-width=\"2\"/>   <line x1=\"298\" y1=\"133\" x2=\"255\" y2=\"185\" stroke=\"red\" stroke-width=\"2\"/>   <text x=\"50\" y=\"125\" stroke=\"black\"> lin1</text>   <line x1=\"100\" y1=\"60\" x2=\"100\" y2=\"185\" stroke=\"red\" stroke-width=\"2\"/>  <line x1=\"250\" y1=\"60\" x2=\"250\" y2=\"185\" stroke=\"red\" stroke-width=\"2\"/>  <line x1=\"105\" y1=\"60\" x2=\"245\" y2=\"185\" stroke=\"red\" stroke-width=\"2\"/>  <line x1=\"245\" y1=\"60\" x2=\"105\" y2=\"185\" stroke=\"red\" stroke-width=\"2\"/>  <text x=\"50\" y=\"280\" stroke=\"black\"> lin2</text>   <text x=\"300\" y=\"280\" stroke=\"red\">1.0 (&quot;Bias&quot;)</text>   <line x1=\"295\" y1=\"285\" x2=\"190\" y2=\"345\" stroke=\"red\" stroke-width=\"2\"/>   <line x1=\"100\" y1=\"210\" x2=\"170\" y2=\"340\" stroke=\"red\" stroke-width=\"2\"/>  <line x1=\"250\" y1=\"210\" x2=\"180\" y2=\"340\" stroke=\"red\" stroke-width=\"2\"/></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| label: fig-xor\n",
    "#| fig-cap: \"A fully-connected (linear) 3-layer network for XOR\"\n",
    "\n",
    "\n",
    "SVG('<svg xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\" width=\"450\" height=\"350\"> \\\n",
    "  <circle cx=\"100\" cy=\"50\" r=\"4\" stroke=\"red\" stroke-width=\"2\" fill=\"none\" />\\\n",
    "  <circle cx=\"250\" cy=\"50\" r=\"4\" stroke=\"red\" stroke-width=\"2\" fill=\"none\" />\\\n",
    "  <text x=\"92\" y=\"205\" font-size=\"24pt\">σ</text>\\\n",
    "  <text x=\"240\" y=\"205\" font-size=\"24pt\">σ</text>\\\n",
    "  <circle cx=\"175\" cy=\"350\" r=\"4\" stroke=\"red\" stroke-width=\"2\" fill=\"none\" />\\\n",
    "  <text x=\"300\" y=\"125\" stroke=\"red\">1.0 (\"Bias\" weights)</text> \\\n",
    "  <line x1=\"295\" y1=\"130\" x2 = \"110\" y2 = \"195\" stroke=\"red\" stroke-width = \"2\"/> \\\n",
    "  <line x1=\"298\" y1=\"133\" x2 = \"255\" y2 = \"185\" stroke=\"red\" stroke-width = \"2\"/> \\\n",
    "  <text x=\"50\" y=\"125\" stroke=\"black\"> lin1</text> \\\n",
    "  <line x1=\"100\" y1=\"60\" x2=\"100\" y2=\"185\" stroke=\"red\" stroke-width=\"2\"/>\\\n",
    "  <line x1=\"250\" y1=\"60\" x2=\"250\" y2=\"185\" stroke=\"red\" stroke-width=\"2\"/>\\\n",
    "  <line x1=\"105\" y1=\"60\" x2=\"245\" y2=\"185\" stroke=\"red\" stroke-width=\"2\"/>\\\n",
    "  <line x1=\"245\" y1=\"60\" x2=\"105\" y2=\"185\" stroke=\"red\" stroke-width=\"2\"/>\\\n",
    "  <text x=\"50\" y=\"280\" stroke=\"black\"> lin2</text> \\\n",
    "  <text x=\"300\" y=\"280\" stroke=\"red\">1.0 (\"Bias\")</text> \\\n",
    "  <line x1=\"295\" y1=\"285\" x2 = \"190\" y2 = \"345\" stroke=\"red\" stroke-width = \"2\"/> \\\n",
    "  <line x1=\"100\" y1=\"210\" x2=\"170\" y2=\"340\" stroke=\"red\" stroke-width=\"2\"/>\\\n",
    "  <line x1=\"250\" y1=\"210\" x2=\"180\" y2=\"340\" stroke=\"red\" stroke-width=\"2\"/>\\\n",
    "</svg>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b3e324-c47b-479e-a0a5-3a6d8af6e9b9",
   "metadata": {},
   "source": [
    "In this figure, the `lin1` lines represent the weights connecting the input nodes (circles) to the nodes where we applied the sigmoid function ($\\sigma$), and the `lin2` lines represent the weights connecting the middle layer's outputs to the single node in the output layer.\n",
    "\n",
    "When every node in a layer is connected to every node in the output layer, it's called a \"fully-connected\" layer or a \"linear\" layer. You can visualize _somewhat_ larger networks including the weight connections:\n",
    "\n",
    "![](media/4layerlinear.png)\n",
    "\n",
    "But drawing all the weights quickly becomes unmanageable. Instead, neural networks are typically described just drawing the layers: \n",
    "\n",
    "![](media/xorblock.svg)\n",
    "\n",
    "Or even just: \n",
    "\n",
    "![](media/xorblock2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b316f-e79a-48b3-bac0-f66807ff2759",
   "metadata": {},
   "source": [
    "When we did the MNIST hand-writing challenge, we switched to a 28x28 black-and-white input, a 500-node middle layer, and a 10-node output layer. To visualize that, we might draw (forgive my isometric clumsiness):\n",
    "\n",
    "![](media/mnistblock_out.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c30aa9-8d0e-46b2-91ee-1e6fa8ad76fe",
   "metadata": {},
   "source": [
    "## 3 Layers Are Theoretically Sufficient, But Often Don't Scale\n",
    "\n",
    "To fully connect a 28x28 input with 500 nodes requires 28x28x500 weights. Add in a bias weight for each middle-layer node, and that's 392,500 weights. \n",
    "\n",
    "Fully connecting the 500 middle-layer nodes to 10 output nodes adds another 5,000 weights. \n",
    "\n",
    "We were still able to train this on small laptops, but the number of middle-layer nodes in a 3-layer neural network tends to increase very fast as the problem gets harder. If we were to try to extend hand-writing recognition from 10 digits to 26 letters (2.6 times harder, perhaps), we might have to increase the number of middle-layer nodes, not to 500x2.6, but maybe to 5000. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e068a1d8-4c7a-4ea6-be4d-32a67ca026a0",
   "metadata": {},
   "source": [
    "## Convolutional Layers\n",
    "\n",
    "What if, instead of the input being the full 28x28 pixel image, you had a 3-pixel by 3-pixel window that you slid over the 28x28 MNIST input and you mapped the result to a single output node?:\n",
    "\n",
    "![](media/convolution.svg)\n",
    "\n",
    "You only have a 3x3 input and a size 1 output, so that's just 9 weights! Add the bias and it's still tiny! Now, you slide the window 28 times in a row and after each row you just slide it down 1, so you're going to end up with an output the same size as your input: \n",
    "\n",
    "![](media/convolution_block.svg)\n",
    "\n",
    "True, the middle layer is bigger than we had with 3 layers (28x28 = 784 nodes vs. 500 nodes), but the number of weights is just 10! \n",
    "\n",
    "This is what's called a \"convolution layer\". In this case, it's a 2D convolution layer because it's sliding a window over-and-down a 2D image. You can imagine that if you had a video file, you could do a 3D convolution where you looked at 3 pixels by 3 pixels by 3 frames. \n",
    "\n",
    "Instead of just having one set of weights (one 3x3 **kernel**) it's common to have several sets of weights. So if instead of 1 **output channel** you have 6, instead of 10 weights, you'd have 60. Still vastly, vastly smaller than the Linear layer!\n",
    "\n",
    "### What do nodes in a convolution layer react to? \n",
    "\n",
    "A single node in a convolution layer only \"sees\" a 3x3 input. (Or 5x5, or 7x7, etc. But it's often 3x3). \n",
    "\n",
    "Imagine that, like our XOR network, the inputs were only 0 or 1 (instead of shades of gray, as they _really_ are in MNIST). And imagine that the weights you had were:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "0.5 & -1.0 & -1.0 \\\\\n",
    "-1.0 & 0.5 & -1.0 \\\\\n",
    "-1.0 & -1.0 & 0.5\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The small set of weights you use when you slide the window over your total input is called the **convolution kernel**. \n",
    "\n",
    "And what we're doing is the same-old, same-old: \n",
    "\n",
    "*    multiply our inputs by our weights,\n",
    "*    summing that, \n",
    "*    and applying an activation function like `tanh` to the result\n",
    "\n",
    "So what are the kinds of value we get from this kernel? Let's say the input is:\n",
    "\n",
    "![](media/convolution_kernel.svg)\n",
    "\n",
    "And we count the black pixels as 1.0 and the blank pixels as 0.0, then our math becomes:\n",
    "\n",
    "$$\n",
    "\\tanh(\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "0.5 \\times 1.0 & -1.0 \\times 0 & -1.0 \\times 0 \\\\\n",
    "-1.0 \\times 0 & 0.5 \\times 1.0 & -1.0 \\times 0 \\\\\n",
    "-1.0 \\times 0 & -1.0 \\times 0 & 0.5 \\times 1.0\n",
    "\\end{array}\n",
    "\\right]\n",
    ") = \\tanh(1.5) = 0.91\n",
    "$$\n",
    "\n",
    "--- \n",
    "\n",
    "Compare that with this input:\n",
    "\n",
    "![](media/conv_input2.svg)\n",
    "\n",
    "The black pixels get multiplied by -1.0, and we end up with $$ \\tanh(-3) = -0.995 $$.\n",
    "\n",
    "In other words, we've created a \"top-left, lower-right diagonal slash\" detector:\n",
    "\n",
    "![](media/convolution_kernel2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed296229-3e03-4c26-a667-9b6b0ddd262b",
   "metadata": {},
   "source": [
    "#### Convolution layers create \"Feature Maps\"\n",
    "\n",
    "When you take a certain input, multiply it by the weights in the kernel, and apply an activation function to the sum, the resulting number (the **activation level** of the **convolutional node**) will be higher for some patterns than for others. If the weights are set \"correctly\" (via slow modification via gradient descent, etc.), the kernel is a **feature detector**. The results of a convolutional layer are often called **feature maps**. \n",
    "\n",
    "The kernel we showed above detects \"upper-left to lower-right diagonals.\" If we ran this over a 28x28 input, we'd have a 28x28 set of activations. Anywhere they were near 1.0, it would indicate that the 3x3 pixel area \"above\" that point \"looked like\" it had a backslash. Where the activation levels are near 0, it would indicate \"nothing like a backslash.\" Even if the input were: \n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "\\blacksquare & \\square & \\square \\\\\n",
    "\\square & \\square & \\square \\\\\n",
    "\\square & \\square & \\blacksquare\n",
    "\\end{array}\n",
    "\\right] \\times \n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "0.5 & -1.0 & -1.0 \\\\\n",
    "-1.0 & 0.5 & -1.0 \\\\\n",
    "-1.0 & -1.0 & 0.5\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The activation would be $$ \\tanh(0.5 + 0.5) = 0.76 $$ \"Kinda' like the feature, but not fully.\"\n",
    "\n",
    "On the other hand, those -1 weights mean that any _other_ black squares in the input will pretty-much guarantee low activation:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "\\blacksquare & \\square & \\blacksquare \\\\\n",
    "\\square & \\square & \\square \\\\\n",
    "\\square & \\square & \\blacksquare\n",
    "\\end{array}\n",
    "\\right] \\times \n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "0.5 & -1.0 & -1.0 \\\\\n",
    "-1.0 & 0.5 & -1.0 \\\\\n",
    "-1.0 & -1.0 & 0.5\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \\tanh(0.5 + -1.0 + 0.5) = \\tanh(0) = 0.0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec44de-c01d-4e81-93c2-a4975615798f",
   "metadata": {},
   "source": [
    "## Deep Feature Maps\n",
    "\n",
    "A **Deep Neural Network** is any Artificial Neural Network (**ANN**) that has more than 3 layers.\n",
    "\n",
    "If you have a first convolutional layer that has a **receptive field** of 3x3 pixels and your convolve over a 28x28 image, you end up with a 28x28 feature map. (You have to pad the edges with 0s or you'd actually end up with a 26x26 map, but it's normal to pad.)\n",
    "\n",
    "What happens when you do a convolution over _that_ feature map? \n",
    "\n",
    "![](media/fmap_2.svg)\n",
    "\n",
    "Remember that \"Feature Map 1\" is a bunch of \"3x3 pixel feature detectors\" . \"Feature Map 2\" also has a 3x3 receptive field, but each of its inputs is a \"3x3 pixel feature detector.\" So you could either call it:\n",
    "\n",
    "* A map of 3x3 detectors of 3x3 features (more accurate)\n",
    "* A map of 9x9 features (less accurate, since it's not looking at every pixel)\n",
    "\n",
    "A third convolutional layer would give us something that was sorta' \"looking at\" a 27x27 area, a fourth one sorta' looking at 81x81 pixels, etc. \n",
    "\n",
    "### Crucial Concept: Deep neural networks move from low abstraction to high abstraction\n",
    "\n",
    "Only the first feature map is reacting to _every single pixel_ in the input. It only \"sees\" very small details and so can only react to very small patterns. \n",
    "\n",
    "The second feature map doesn't see every single pixel, but it sees every single very small pattern in the first feature map. Many layers deep, the weights are evaluating very abstract features.\n",
    "\n",
    "![](media/abstraction.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1fb358-0576-44ca-8e51-cce25f240969",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "You could certainly fit many 28x28 convolutional layers into your computer memory, but, in most cases, you're not really paying attention to _every_ 3x3 pixel block, you only want to pay attention to \"interesting\" ones. If your weights are set to their trained values, \"interesting\" inputs will be those that generate values closer to -1 or 1 rather than 0. (Or, with other activation functions, maybe those with values closer to 0 or 1 rather than 0.5.)\n",
    "\n",
    "Convolutional layers are generally paired with a **Pooling layer** that also slides a window over the input. This time, though, the **receptive field** is generally just 2x2. \n",
    "\n",
    "And instead of having anything to do with weights, the pooling layer just selects, from among its inputs, the one with the greatest magnitude: \n",
    "\n",
    "![](media/pooling_kernel.svg)\n",
    "\n",
    "A pooling layer with a 2x2 receptive field cuts the size of its input layer to a quarter of it's original size (a half in each direction). One with a 3x3 receptive field cuts it by 8/9s! (2/3s in each direction). \n",
    "\n",
    "Pooling layers are very commonly used with convolutional layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c324fd-3105-4928-be88-a81d8e27f3ad",
   "metadata": {},
   "source": [
    "## A Deep Convolutional Neural Network for MNIST\n",
    "\n",
    "I don't have time to go over this step by step, so I'm just going to present the code quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f64b5ac-0d2d-4927-96ec-85ef6ebb32cf",
   "metadata": {},
   "source": [
    "This is not a _very_ deep neural network, lol. It's really just 6 layers:\n",
    "\n",
    "1) Input layer (28 x 28 pixels, only 1 byte per pixel: 1x28x28)\n",
    "2) Feature Map 1 (5x5 receptive field, 28 x 28 features, 16 kernels: 16x28x28)\n",
    "3) Pooling layer 1 (Reduces size to 16x14x14)\n",
    "4) Feature Map 2 (5x5 receptive field, 14 x 14 features, 32 kernels: 32x14x14)\n",
    "5) Pooling layer 2 (Reduces size to 32x7x7)\n",
    "6) Classification layer (Fully connects pooling layer 2 to 10 output values: 1568x10)\n",
    "\n",
    "![](media/mnist_conv.svg)\n",
    "\n",
    "It has fewer than 30,000 weights, that's 1/10th the size of our 3-layer MNIST neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf7f16-c953-4174-8ca4-2334132b73c6",
   "metadata": {},
   "source": [
    "You'll see that, just as we defined a new class for our Xor network and our 3-layer MNIST network, we're doing the same here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90e9ee-f1f2-4354-91be-757e3a30f0e8",
   "metadata": {},
   "source": [
    "To make the code a little shorter, I define `conv1` and `conv2` as layers that themselves each have a convolutional layer (`Conv2d`), use the `ReLU` activation function, and a pooling layer (`MaxPool2D`). Creating **blocks** like this that internally have layers and which, in turn, you layer with other blocks, is how deep-learning **architectures** are built. \n",
    "\n",
    "### Parameters to `Conv2d`\n",
    "\n",
    "When defining a convolutional layer (`Conv2d`) in Pytorch, the depth of the input is the number of `in_channels`. With a black-and-white image, we only have 1 input per pixel. If it were an RGB image with 3 values defining the pixel, the `in_channels` would be 3. \n",
    "\n",
    "The `out_channels` is the number of kernels you want. Generally, you want to train a bunch of kernels, since each kernel has a receptive field of only `kernel_size * kernel_size` weights. (And remember that you'll also have 1 bias weight for each output.) \n",
    "\n",
    "Although we talked about 3x3 pixel stuff above, I happen to know that it takes a really long time to train a 3x3 convnet on MNIST. Instead, we use a `kernel_size` (really \"length of one side of kernel\") of 5. \n",
    "\n",
    "How much do you slide the window at each step? We set `stride` to 1, meaning that we just move the window 1 pixel over until we hit the end of the row and then go down 1. Setting it to a higher number will miss some of the small-scale patterns in your input, but will make for a smaller feature map. (I'm not sure I've _ever_ seen a production system where `stride` > 1 was used.) \n",
    "\n",
    "What happens when your window hits the edge? Generally, you use `padding` to just put 0s on \"the outside.\" Since our `kernel_size` is 5, that means that one when the middle pixel is on the edge, there are 2 pixels \"outside.\" Thus, `padding = 2`. \n",
    "\n",
    "### Parameters to `MaxPool2d`\n",
    "\n",
    "If you understood the discussion of pooling layers, this should be pretty clear: our pooling layer has a receptive field of 2x2. It cuts the size of the feature map by 3/4, saving only the activation that has the highest absolute value. \n",
    "\n",
    "### Parameters to `Linear`\n",
    "\n",
    "The convolutional layers detect the features of the input, but we need to map those into one of 10 output classes (the digits from 0-9). Going through the \"shapes\" of the input size, the convolutional and pooling layers, the output of the second pooling layer is [32,7,7] meaning that I need 32x7x7 weights for each of the 10 outputs (plus 1 bias weight for each output value). We create a fully-connected, aka Linear, layer: 32 x 7 x 7 x 10 + 10 = 15,690 weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57356f8-200d-46c4-b49b-70508fa577b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f49c6-3c9f-4798-9e21-e5e30a9afd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.backends.cuda.is_built() else torch.device('mps') if torch.backends.mps.is_built() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98cb680-3b41-490a-9d7e-abeaccb4391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13d848b-e977-46df-8420-4f763ba53d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true \n",
    "#| code-fold: false\n",
    "\n",
    "class MnistConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistConv, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels = 1, # Black-and-white, single value\n",
    "                out_channels = 16, \n",
    "                kernel_size = 5,\n",
    "                stride = 1,\n",
    "                padding = 2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        # After pooling, output is 14x14x6 \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels = 16,\n",
    "                out_channels = 32,\n",
    "                kernel_size = 5,\n",
    "                stride = 1,\n",
    "                padding = 2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2)\n",
    "        )\n",
    "        # After pooling, output is 7x7x32\n",
    "        self.out = nn.Linear(7*7*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # Flatten the output shape of conv2 to shape of output layer. Let PyTorch do the calc.\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "net = MnistConv().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd763c-7404-4165-b955-c3aa4d49f956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             416\n",
      "              ReLU-2           [-1, 16, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
      "            Conv2d-4           [-1, 32, 14, 14]          12,832\n",
      "              ReLU-5           [-1, 32, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
      "            Linear-7                   [-1, 10]          15,690\n",
      "================================================================\n",
      "Total params: 28,938\n",
      "Trainable params: 28,938\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.32\n",
      "Params size (MB): 0.11\n",
      "Estimated Total Size (MB): 0.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#| echo: true \n",
    "#| code-fold: false\n",
    "\n",
    "\n",
    "torchsummary.summary(net, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced573c-4cbb-44f0-9a1c-0080c2499728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72622f-79bf-4924-8c57-027dd40de4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b8255-118f-4ea4-b97e-acefdb5b6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83942909-5cf9-4deb-8f46-0ded6317ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2565ae-9977-49b5-b86a-3100dee4d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "trainset.data = trainset.data.to(device)\n",
    "testset.data = testset.data.to(device)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae2cf3-42be-4fb0-9dd1-f7666489a92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa3d47-bfb7-4af9-98e0-406691c78ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e74e37-1785-4475-94c3-a7e48321c8e3",
   "metadata": {},
   "source": [
    "This is the _exact same_ training loop as we used on 3-layer MNIST. Not a line of code is different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a191d-1ea4-4e8f-969f-c50a995918b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      8\u001b[0m epoch_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/detectron2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mambaforge/envs/detectron2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/mambaforge/envs/detectron2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/mambaforge/envs/detectron2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/mambaforge/envs/detectron2/lib/python3.9/site-packages/torchvision/datasets/mnist.py:142\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index])\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "#| echo: true \n",
    "#| code-fold: false\n",
    "\n",
    "epochs = 2\n",
    "losses = []\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    epoch_start = time.time()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        losses.append(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        seconds = time.time() - epoch_start\n",
    "    print(f\"Epoch {epoch} : loss = {running_loss/len(trainset):.6f}, took {seconds:.1f} seconds\")\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447ffa8-523d-4d02-892e-ce6012f99bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_for_image(net, q_image):\n",
    "    net.eval()\n",
    "    outputs = net(q_image.unsqueeze(0))\n",
    "    # outputs has a batch dimension, so get the values at dim 0\n",
    "    vals = outputs.squeeze().detach().numpy()\n",
    "    index_of_max_val = np.argmax(vals)\n",
    "    labels_for_ix = { \n",
    "        0 : \"Zero\",\n",
    "        1 : \"One\",\n",
    "        2 : \"Two\", \n",
    "        3 : \"Three\", \n",
    "        4 : \"Four\", \n",
    "        5 : \"Five\", \n",
    "        6 : \"Six\", \n",
    "        7 : \"Seven\",\n",
    "        8 : \"Eight\",\n",
    "        9 : \"Nine\"\n",
    "    }\n",
    "    return labels_for_ix[index_of_max_val]\n",
    "\n",
    "#prediction_for_image(net, query_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c7d931-278c-4a75-ba6f-5dd96349b5d4",
   "metadata": {},
   "source": [
    "You may remember this hard-to-get target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74109748-fb7d-4c7b-81b7-2c9548d335cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| code-fold: false\n",
    "\n",
    "def show_and_tell(net, dataset, index):\n",
    "    query_image, ground_truth = dataset[index]\n",
    "    \n",
    "    # The image is now a PyTorch Tensor, so you need to convert it to a NumPy array first\n",
    "    image = query_image.numpy()\n",
    "    \n",
    "    # The data is a 1x28x28 array, you need to remove the extra dimension\n",
    "    image = image.squeeze()\n",
    "    \n",
    "    # Display the image and its label\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title('Label: %i' % ground_truth)\n",
    "    plt.show()\n",
    "\n",
    "    prediction = prediction_for_image(net, query_image)\n",
    "    print(f\"Prediction is \\\"{prediction}\\\", ground truth is \\\"{ground_truth}\\\"\")\n",
    "\n",
    "# testset[478] is a hard one! Your model may get it wrong!\n",
    "show_and_tell(net, testset, 478)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a82748-afbc-4ab6-9352-622a950fb5c5",
   "metadata": {},
   "source": [
    "## What's our overall accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb98314-cce4-4df6-9abe-b50b39c23fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| code-fold: false\n",
    "\n",
    "loss = np.array([])\n",
    "net.eval()\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, ground_truths = data\n",
    "    predictions = net(inputs)\n",
    "    batch_loss = torch.eq(predictions.argmax(dim=1), ground_truths.argmax(dim=0)).float()\n",
    "    loss = np.append(loss, batch_loss.detach().numpy())\n",
    "accuracy = 1.0 - np.mean(loss.flatten())\n",
    "print(f\"Across {len(testloader) * batch_size} test images, accuracy is {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f42ed-9acb-4895-8ef2-5101143f9a67",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "Although neural networks essentially consist of huge numbers of weights interconnecting layers, it's impossible to diagram the interconnections. Instead, neural networks are discussed using block diagrams, with the connections implied.\n",
    "\n",
    "Although 3-layer neural networks are theoretically able to compute any computable function, the number of weights needed scales poorly. \n",
    "\n",
    "For image (and audio) (and sequence prediction), **convolutional layers** slide a window over the input, creating a **feature map** from very few weights. Because there are so few weights, you generally train several different **kernels** at once, generating several **output channels**. (For instance, the first convolutional layer in the MNIST network generates 16 channels.)\n",
    "\n",
    "**Pooling layers** select the strongest signal in their **receptive field**, discarding the rest. This reduces the size of the layers. \n",
    "\n",
    "The \"deeper\" you go in a neural network, the harder it is to map a node's value into a particular value in the input. What makes the value high? Well, the weights coming in and their inputs. What set those inputs? Well, the weights coming into _that_ layer and _their_ inputs. etc. \n",
    "\n",
    "By the time you're more than a handful of layers in, most of the pixels in the input have _something_ to do with the activation of every node in a layer. This makes it very difficult to audit a neural model to make sure it's making predictions \"the right way,\" or \"using the right data.\" This is known as the **Interpretability Problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a3282-0f32-476b-ab0e-a8cd3d1dea5d",
   "metadata": {},
   "source": [
    "In the case of MNIST, the convolutional neural network performs with essentially the same accuracy as the 3-layer neural network: generally achieving 92-93% accuracy. This is despite having fewer than 10% of the weights, which effectively means using much less memory. On the other hand, this deeper neural network trains a little slower, as convolving (sliding the window) is an inner loop that has to happen (even if we don't have to explicitly write the code). The depth also means that the code has a little less **cache coherence**, which is a factor in performance tuning.\n",
    "\n",
    "Pragmatically, the weight/memory savings *vastly* outweighs the slight speed hit. Convolutional neural networks are the standard architecture for neural network-based vision applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3bc1d6-0eba-45cc-89cf-4e611fa19b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "detectron2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
